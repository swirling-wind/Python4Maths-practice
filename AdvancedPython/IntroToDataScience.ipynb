{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A basic introduction to Data Science\n",
    "**Disclaimer:** This is not a full introduction and unashamedly focusses on the areas I'm most interested in (optimisation). We are going to spend a small amount of time looking at howto extract data contained in text files. The focus is then on giving you an introduction to how some of the machine learning algorithms work under the hood (rather than just calling the existing implementations from a library). Overall this is very basic though, with a thorough treatment requiring at least a semester course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "In this exerercise we are going to work through a fairly common scenario. After developing a couple of algorithms for solving a particular problem (or implementing some \"standard\" algorithms as well as your shiny new and hopefully improved algorithm), you test the algorithm on some instances and want to know which algorithm is \"best\"? Or which algorithm is best for what type of data? \n",
    "\n",
    "Here we are going to go through this type of analysis by focussing on alternative linear programming algorithms. So here are the elements that go into the analysis:\n",
    "* The problem to be solved:  $\\min c^T x$ subject to $A x = b,\\ x \\ge 0$ where $A$ is a $m\\times n$ matrix, $x$ is a vector of decision variables and $b$ and $c$ are appropriate constant vectors. In practice we normally allow some variants (for example inequality constraints, arbitrary lower and upper bounds on the variables). \n",
    "* Algorithms: Three standard algorithms, *primal* simplex, *dual* simplex, and the *barrier* method, as implemented by the CPLEX solver library\n",
    "* Instances: The [MIPLIB2010](http://miplib2010.zib.de/) \"standard\" test instances. The data sets are actually for mixed integer problems (where some variables are required to be binary or general integers), but for these tests we will throw away the binary requirements. There are 87 instances in this data set\n",
    "\n",
    "To make things easy you will not be required to re-run all of the tests but are given the results from running each algorithm on each instance already pre-processed out of the log files from individual runs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with just the data\n",
    "To make things easy, here is a way to just start with the data you should have by now (you can also skip the above and just start from here).\n",
    "As a reminder on using data frames, this [pandas selection tutorial](https://medium.com/dunder-data/selecting-subsets-of-data-in-pandas-6fcd0170be9c) provides a good summary on how to access rows,columns etc \n",
    "\n",
    "Also to make things easy there is a version of the data that has\n",
    "* All timing (tick) information for the different algoritms removed\n",
    "* Replace the min/max value of coefficients with the magnituded (log) of the range\n",
    "* Scaled all columns to have entries in the range $[-1,1]$ (to have similar magnitudes)\n",
    "\n",
    "You can load the raw data (including 'PrimalT', 'DualT' and 'BarrierT' performance fields) and the preprocessed features using the commands below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from urllib.request import urlopen\n",
    "data = pandas.read_json(urlopen(\"http://users.monash.edu/~andrease/Files/Other/cplexLP.json\"))\n",
    "features = pandas.read_json(urlopen(\"http://users.monash.edu/~andrease/Files/Other/features.json\"))\n",
    "features.head(5) # show first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For the much of the analysis we are only interested in the feature columns that _exclude_ the runtime (ticks) with the different methods (since these are an output rather than an input of the algorithms). We can get a copy of our data that excludes the runtime using\n",
    "```python\n",
    "features = data[[key for key in data if not key.endswith(\"T\")]].copy()\n",
    "```\n",
    "We might also want to replace the various max/min elements with something more sensible such as the log of the max and min or log of the range (since it is more likely that this magnitutde has an impact than the absolute values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# back up the data so we can reload it easily when required\n",
    "open(\"cplexLP.json\",\"w\").write(data.to_json())  # or pick your favourite data format...\n",
    "print('Reload using: data = pandas.read_json(open(\"cplexLP.json\",\"r\"))')\n",
    "# Preprocessing of data to make it more easily usable\n",
    "data = pandas.read_json(open(\"cplexLP.json\",\"r\")) # reload data\n",
    "\n",
    "features = data[[key for key in data if not key.endswith(\"T\") and not key[-3:] in [\"Max\",\"Min\"]]].copy()\n",
    "for f in data:\n",
    "    if f.endswith(\"Max\"):\n",
    "        features[f.replace(\"Max\",\"Rng\")] = np.log(np.maximum(\n",
    "            data[f].values-data[f.replace(\"Max\",\"Min\")].values,np.ones(len(data[f].values))*1e-10))\n",
    "features[\"ConNZ\"] = np.sqrt(features[\"ConNZ\"].values)\n",
    "# scale everyting to be within [-1,1]\n",
    "features /= np.max(features,axis=0)-np.min(features,axis=0)\n",
    "open(\"features.json\",\"w\").write(features.to_json()) \n",
    "print('Reload using: features = pandas.read_json(open(\"features.json\",\"r\"))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising our data\n",
    "This is quite high dimensional. How do we make sense of this?\n",
    "Let's try to do a plot. For this we need to reduce the data to two dimensions. Enter PCA (Principal Component Analysis).\n",
    "For most of the \"machine learning\" type algorithms in this workshop we will use the [scikit-learn](https://scikit-learn.org/stable/documentation.html) set of libraries. This inlcudes a [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) method. However so we can see more easily what is going on here, let's do it directly using the numpy matrix routines.\n",
    "\n",
    "Useful functions:\n",
    "* `data.values` gives the whole frame as a matrix. To get sensible results we want to (a) ignore the timing columns (those ending with T) and (b) replace the max/min of various values by the log of the range (difference between max and min);  (c) use the squareroot of ConNZ, since this is expected to be proportional to VarCnt * ConCnt\n",
    "* We need to form the covariance matrix (subtract the column mean, multiply by the transpose and divide by n-1): $cov= \\frac{1}{n-1}(X-I\\bar x)^T (X-I\\bar x)$. For this you need `np.mean` which takes an argument `axis=d` to take the mean in the d'th dimension, and `X.T` is the transpose of numpy matrix `X`. Or just call `numpy.cov` - but be careful not to get features & observations (columns and rows) mixed up\n",
    "* `np.linalg.eig(X)` returns the (eigenvalues, eigenvectors) as vector/matrix respectively. \n",
    "* We only want to keep the two largest eigenvectors and plot the projection onto these two axes\n",
    "* I'm assuming you are using matplotlib (`matplotlib.pyplot.scatter`) but you can use anything else for plotting\n",
    "\n",
    "Question: Is the scaling of the different attributes proposed above sufficient? Shoud we do something more radical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# INSERT YOUR CODE YERE#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For alternative, more complicated ways to map data down to 2 dimensions there are plenty of other tools in sklearn library (look at [Manifold Learning](https://scikit-learn.org/stable/modules/manifold.html)). Looking at underlying algoirthms is well beyond the scope of this tutorial.  However you may want to have have a go at a few of these such as `TSNE` or `MDS`. Note that some of these manifold learning methods are randomised so won't return the same result each time.\n",
    "\n",
    "Basic usage is something like\n",
    "```Python\n",
    "pts = TSNE(n_components=2).fit_transform(feastures)\n",
    "# pts is an n x 2 matrix of the n points mapped into 2 dimensions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE, MDS,LocallyLinearEmbedding,SpectralEmbedding\n",
    "\n",
    "# TEST YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning - clustering\n",
    "In unsupervised learning we are looking for some structure without having any real idea of what is \"correct\". The simplest case of unsupervised learning is clustering - grouping data into clusters of points that are close together. \n",
    "\n",
    "The simplest way to do this is to apply the `cluster.KMeans` model from the `scklearn` library. See this [tutorial](https://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html) for more information.\n",
    "Try splitting the data (based on the features table) into 3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "\n",
    "### INSERT CODE HERE  ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing our own k-means algorithm\n",
    "The basic algorithm is very simple - so simple that we can write our own\n",
    "```\n",
    "Pick an initial assignment of points to clusters (or an initial set of centres of the clusters)\n",
    "Repeat until converged:\n",
    "    Assign each point to the nearest centre (euclidean distance)\n",
    "    Move the center of each cluster to the centroid (average of all points in the cluster)\n",
    "```\n",
    "\n",
    "This method is a heuristic: It is not guaranteed to give an optimal solution (the one with the least average distance of points to their centre) and it will converge to different solutions depending on the initial solution we start with.\n",
    "\n",
    "Some usuful numpy funtions\n",
    "* `np.random.randint(low,high,n)` returns array length n of integers in `range(low,high)`\n",
    "* `np.mean(matrix,axis=d)` = array of means (along either columns or rows depending on axis\n",
    "* `np.linalg.norm` = norm of an array\n",
    "* `np.argmin(matrix,axis=d)` = like mean iterates over elements along just one axis and returns the minimum index\n",
    "* `X[v > 0]` - returns a submatrix of X depending on which elements of `v` are positive. Whether `v` may be of length number of rows (to select whole rows) or same dimension as X (to select submatrix). Similarly `X[:,v>0]` would select submatrix based on columns where `v` is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def kmeansHeur(data,p):\n",
    "    \"\"\"Given a n x m matrix with n points and m features \n",
    "    return list/np.array length n of labels (in range(p))  \n",
    "    \"\"\"\n",
    "    data = np.array(data) # just to make convert from any other data type\n",
    "    n,m = data.shape\n",
    "    lbl = np.random.randint(0,p,n) # assign a random label from 0 to p-1\n",
    "    ### INSERT YOUR CODE HERE ####\n",
    "    return lbl\n",
    "\n",
    "ncluster = 3\n",
    "lblsHeur = kmeansHeur(features,ncluster) # fit model to data\n",
    "print(\"Points per cluster=\",[sum(1 for i in lblsHeur if i==k) for k in range(ncluster)])\n",
    "\n",
    "## ADD CODE TO PLOT POINTS COLOURED BY LABEL:\n",
    "# plt.scatter( <XVALUES>, <YVALUES>, c=lblsHeur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This answer is probably not the same that you got wiht KMeans from sklearn. Which one is \"right\" or at least \"better\"? Need to formally define the objective function that we are trying to optimise. Essentially we are minimising the sum of squared norm distances:\n",
    "$$\\min_{c_k,C_k} \\sum_k \\sum_{i\\in C_k} ||c_k - x_i||^2$$\n",
    "Where $c_k$ and $C_k$ are the centre of cluster $k$ and the set of points in the cluster respectively. Each $x_i$ is a point of the data.\n",
    "\n",
    "Compute the objective function for both your solution and the KMeans solution. Which one is better? Is either of these optimal (and how would we know?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterObj(data,label):\n",
    "    \"Input: feature matrix (pts x features) and label for each feature.\"\n",
    "    data = np.array(data)\n",
    "    P = set(label) # unique labels\n",
    "    centre = [np.mean(data[label==k],axis=0) for k in P]\n",
    "    return sum(np.linalg.norm(data[i,:]-centre[k])**2  for i,k in enumerate(label))\n",
    "\n",
    "# compare the two objectives: clusterObj(features,k_means.labels_),clusterObj(features,lblsHeur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our method is a randomised heuristic and the result depends on the starting point. Rerun your `kmeansHeur` repeatedly (say 500 times) and keep the best result. How does this compare to the result from a single run of `sklearn.cluster.KMeans` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact optimisation\n",
    "Just to show that finding the best solution for clustering, even with such a small data set, is not trivial, here is a bit of code that uses the Mixed Integer Quadratic Programming solver from CPLEX. This can be ignored as it is not effective (need a much smarter formulation and/or custom algorithm to get a provably optimal solution). \n",
    "Documentation of the optimisation engines available via the [docplex](http://ibmdecisionoptimization.github.io/docplex-doc/) library (academic license availalbe and installed on maxima). The [examples](https://github.com/IBMDecisionOptimization/docplex-examples/tree/master/examples/cp/jupyter) provided by IBM include many jupyter notebooks.  An overview of [constraint types](http://ibmdecisionoptimization.github.io/docplex-doc/cp/docplex.cp.modeler.py.html#module-docplex.cp.modeler) available is found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical programming optimisation model\n",
    "import docplex.mp.model as MIP\n",
    "D = np.array(features)\n",
    "mdl = MIP.Model() # create mixed integer programming model\n",
    "N,P = range(len(lblsHeur)),range(3)\n",
    "rng = np.max(D,axis=0)-np.min(D,axis=0)\n",
    "x = [mdl.binary_var_list(len(P)) for i in N] \n",
    "y = [mdl.continuous_var_list(D.shape[1],lb=0,ub=rng) for i in N]\n",
    "z = [mdl.continuous_var_list(D.shape[1],lb=np.min(D,axis=0),ub=np.max(D,axis=0)) for k in P] # center\n",
    "# assign each point to a cluster with each cluster non-empty\n",
    "for i in N: mdl.add( mdl.sum(x[i][k] for k in P) == 1)\n",
    "for k in P: mdl.add( mdl.sum(x[i][k] for i in N) >= 1)\n",
    "Mn,Mx = np.min(D,axis=0),np.max(D,axis=0)\n",
    "for i in N:\n",
    "    for k,M in enumerate(rng):\n",
    "        for j in P:\n",
    "            mdl.add( y[i][k] >= z[j][k] - D[i,k] - (Mx[k]-D[i,k])*(1-x[i][j]))\n",
    "            mdl.add( y[i][k] >= D[i,k] - z[j][k] - (D[i,k]-Mn[k])*(1-x[i][j]))\n",
    "#F = range(len(rng))\n",
    "#for i in N:\n",
    "#    for j in N[i+1:]:\n",
    "#        for k in P:\n",
    "#            for f in F:\n",
    "#                mdl.add(y[i][f]+y[j][f] >= abs(D[i,f]-D[j,f])*(x[i][k]+x[j][k]-1))\n",
    "mdl.minimize(\n",
    "    mdl.sum( y[i][k]**2 for k in range(len(rng)) for i in N)\n",
    ")\n",
    "CPXparam = mdl.context.cplex_parameters\n",
    "CPXparam.timelimit = 20\n",
    "CPXparam.threads = 2\n",
    "#for p in [CPXparam.timelimit, CPXparam.threads]: print(p.get_qualified_name(),\" = \",p.get())\n",
    "mdl.solve(log_output=True) # log_output = write to screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we make this go faster?  Perhaps pass in a solution we have found already - just need to define the binary variables. Essential functions\n",
    "* `soln = docplex.mp.solution.SolveSolution(mdl)` create a solution object for the model\n",
    "* `soln.add_var_value(x[i][j],1)` sets the given variable to 1\n",
    "* `soln.check_as_mip_start()` Is this a valid solution?\n",
    "* `mdl.add_mip_start(soln)` Use the solution as a warm start for the optimisation\n",
    "* `mdl.solve()` start/continue the optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docplex\n",
    "### INSERT CODE HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can arbitrarily assign at least one point to a cluster - since renumbering the clusters gives us equivalent solutions, this just breaks the symmetry.\n",
    "Furthermore the two points furthest appart are definitely going to be in different clusters. So assign these two points to different clusters (actual cluster numbers should match the heuristic solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxD,max_i,max_j = max( (np.linalg.norm(D[i,:]-D[j,:]),i,j) for i in N for j in N[i+1:])\n",
    "print(\"Max distance %d <-> %d = %f\"%(max_i,max_j,maxD))\n",
    "mdl.add(x[max_i][lbls[max_i]]==1)\n",
    "mdl.add(x[max_j][lbls[max_j]]==1)\n",
    "mdl.add_mip_start(soln)\n",
    "CPXparam.mip.strategy.heuristicfreq=1\n",
    "#CPXparam.mip.strategy.rinsheur=1\n",
    "soln = mdl.solve(log_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to give up on this idea - it might well run for hours without finding a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning - Classification with Support Vector Machines\n",
    "We might want to figure out which algorithm works fastest for a given problem. That is, given a linear program and its characteristics, can we decide which algorithm to use to get the solution as fast as possible. Support vector machines are one model for such a classification task. It supposes that there is a _linear_ function of the factors that determines which of 2 classes a point belongs to. That is, that there exists a set of weights $w_i$ and constant $W$ such that\n",
    "$\\sum_i w_i f_i < W$ if one algorithm is faster and $\\sum_i w_i f_i > W$ when the other method is faster where $f_i$ are the features of the instance we are interested in solving. How do we decide what the $w_i$ (and $W$) should be? This is supervised learning where we are using existing _training data_ to fit the parameters. \n",
    "\n",
    "Let's start by trying to work out for which instances the primal vs dual simplex method is faster. We have `primalT` and `dualT` that tells us which is faster for the test data. So we want to find a vector $w$ and constant $W$ such that\n",
    "$\\sum_i w_i f_{ki} \\le W-1$ if `PrimalT` < `DualT` and $\\sum_i w_i f_{ki} \\ge  W+1$ otherwise. (Where $f_{ki}$ is the i-th feature of instance k.  Finding such a set of $w_i$ and $W$ is just a linear programming problem. Some things we need to consider is: The limit of 1 is just to ensure we get some minimum separation (we could pick any number as multiplying each row by a constant would not change the problem other than to increase the non-zero difference between the LHS and W\n",
    "* If the instances can be separated, we want to make the 2 hyperplanes (defined by the $\\pm1$ on the right hand side) as far apart as possible(so that we get a clean separation). The separation distance is $2/||w||$. So maximising the separation is equivalent to $\\min ||w||^2$ which gives us a quadratic program with linear constraints.\n",
    "* It may not be possible to separate the points cleanly into two groups with a single hyperplane. Hence we might want to penalise any point that is mis-classified, perhaps based on how far it is away from the hyperplane. Let $v_k$ be the distance that point $k$ is away from the hyperplane then we might want to $\\min ||w||^2 + \\sum_k v_k$ with $\\sum_i w_i f_{ki} \\le W-1+v_k$,\n",
    "* We can put the two objectives together - by noting that maximisation is the same as minimising the negative and placing greater priority on minimising violations:\n",
    "$\\min ||w||^2 + \\alpha \\sum_k v_k$ subject to $\\sum_i w_i f_{ki} \\le W+v_k-s$ for $k$ such that the primal solution is better (with correspoding $\\ge$ constraints for the other points). Here $\\alpha$ is an arbitrary weight that provides a trade-off between violations (misclassifications) and the distance separating the hyperplanes.\n",
    "* Some variants of this are possible. For example we could replace the$||w||^2$ term by $\\max_i |w_i|$ (for example) which can be written in a linear program, or even drop it entirely. When there are violations these are minimised  if $||w||$ is minimised so we may not need this.\n",
    "\n",
    "Hence for an initial experiment we want to set up an SVM training function that takes our features as input data together with the list instances for which `data['PrimalT'] < data['dualT']` (this python expression returns a boolean column). Let `I` be those instances and `IC` be the complement. The we want to solve the following linear program:\n",
    "$$\\min_{v,w,W} \\sum_{i\\in I\\cup IC} v_i$$\n",
    "$$s.t.\\ \\sum_{k\\in F} D_{if} w_f \\le W - 1 + v_i\\quad\\forall\\ i\\in I$$\n",
    "$$\\quad \\sum_{k\\in F} D_{if} w_f \\ge W + 1 - v_i\\quad\\forall\\ i\\in IC$$\n",
    "$$ v_i \\ge 0\\quad\\forall\\ i$$\n",
    "Here $D_{if}$ is the data for instance $i$ and feature $f$ out of the set of features $F$.\n",
    "\n",
    "How to set this LP up using the CPLEX solver:\n",
    "* `import docplex.mp.model as MP`\n",
    "* `mdl = MP.Model()` create a mathematical programming (optimisation) model \n",
    "* `x = mdl.binary_var_list(n)` create list of variables `x[0]`...`x[n-1]` that are all in {0,1} - not needed here\n",
    "* `x = mdl.continuous_var_list(n,lb=-mdl.infinity,ub=3*np.ones(n))` create list of variables $-\\infty < x[i] \\le 3$ for i=0,...,n-1\n",
    "* `mdl.add( mdl.sum(x[i] for i in range(n)) == 1)` add the constraint $\\sum_{i=0}^{n-1} x_i=1$\n",
    "* `mdl.minimize( x[0]+x[1]**2)` set the objective (linear + quadratic term). Note: CPLEX handles linear and quadratic programming problems but not more general non-linear optimisation problems\n",
    "* `CPXparam = mdl.context.cplex_parameters` access parameters - complete list of [CPLEX parameters](https://www.ibm.com/support/knowledgecenter/SSSA5P_12.8.0/ilog.odms.cplex.help/CPLEX/Parameters/topics/introListAlpha.html). It may be a good idea to set `CPXparam.timelimit=60` seconds (to stop it running too long if the problem is not set up correctly), `CPXparam.threads=1`. This model should run in no more than a second though.\n",
    "* `solution = mdl.solve(log_output=True)` does the optimisation and shows logs some information to the screen as it runs. `solution == None` if the solver fails to obtain a solution.\n",
    "* `mdl.get_solve_status()` gives the final solution status (as a string)\n",
    "* `solution.get_value(x[0]+x[1])` would return the final value of `x[0]+x[1]` in the solution. Alternatively you can also use `solution[x[2]]` to get the value of `x[2]` in the solution. (More informmation on the solution class see [SolveSolution](http://ibmdecisionoptimization.github.io/docplex-doc/mp/docplex.mp.solution.html#docplex.mp.solution.SolveSolution))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical programming optimisation model\n",
    "import docplex.mp.model as MP\n",
    "\n",
    "def SVM(features,label):\n",
    "    \"\"\"features: the usual matrix of points x features\n",
    "    label: a list/array (length number of points) of bools/integers that\n",
    "           identifies the points to be separated from the rest\n",
    "    Returns: array length n.features+1 of w_i plus the constant W\"\"\"\n",
    "    label = np.array(label) # to make sure we are not dealing with DataFrames\n",
    "    #### INSERT YOUR CODE HERE #####\n",
    "    return [solution.get_value(w[f]) for f in F]\n",
    "soln = SVM(features,data['PrimalT']<data['DualT'])\n",
    "print(\"Weights =\",soln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do to improve the fit? How to deal with non-linear separations? Try adding additional \"features\" that capture the non-linear effects. For example we might expect that the effectiveness depends on the size or density of the constraint matrix (density = 'ConNZ' / ('VarCnt' x 'ConCnt')). Create an extended feature set and try again.\n",
    "The idea here is that if we have say a two dimensional set of features (x,y for each point) and we wanted to separate those points that are inside an elipse centered at the origin from those outside, then we can find such a separating elipse by including features $x^2$ and $y^2$ with the optimisation choosing some combination $w_x x^2 + w_y y^2 \\le W$ giving us an elipse while still solving a lienar problem (since the $x^2$ is just a constant coefficent for the variable $w_x$ in the optimisation).\n",
    "\n",
    "Try creating some additional features. Remember for `DataFrames` we can easily create new columns by doing arithmetic with whole columns. For example `features['eqSq'] = features['ConEqual']**2` would create a new column `eqSq` that contains the number of equality constraints (`ConEqual`) squared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE SOME NEW FEATURES HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How well is our classifier doing?\n",
    "To analyse the performance we should think about:\n",
    "* Do we care about:\n",
    "    1. how far we are on the wrong side of the line? (this is what we are optimising at the moment)\n",
    "    2. how many instances are classified incorrectly?\n",
    "    3. how much extra compute time we would incur if we used the minimum?\n",
    "* How well it works for data it hasn't seen?\n",
    "    \n",
    "Below compute some alternative measures of the quality of the fit. Then address the second issue by using a random split of the instances to create 4 groups (of about 21 each). Then we used 3 parts as the \"training\" data to fit the SVM model, and the other group for testing to validate that the model is acutally OK on data that wasn't used for the training. By repeating this 4 times for each group we can compute a more accurate average performance of the approach on this type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMviolation(feat,w,lower=data['PrimalT'],higher=data['DualT']):\n",
    "    \"\"\"Input: feat = matrix of features, w - list/array of weights (length features + 1), \n",
    "     higher/lower = performance measure\n",
    "     Returns total violation of sum( w[i]*feat[i]) <= W or >= W depending on if lower[i] < or > higher[i]\"\"\"\n",
    "    feat = np.array(feat) # just to make sure it is not a DataFrame\n",
    "    v = feat.dot(np.array(w[:-1])) - w[-1]*np.ones(feat.shape[0])\n",
    "    L = [ i for i,(low,high) in enumerate(zip(lower,higher)) if low < high]\n",
    "    H = [ i for i,(low,high) in enumerate(zip(lower,higher)) if low > high]\n",
    "    return sum( max(0,v[i]) for i in L)+sum(max(0,-v[i]) for i in H)\n",
    "def SVMextraT(feat,w,lower=data['PrimalT'],higher=data['DualT']):\n",
    "    \"\"\"Input: feat = matrix of features, w - list/array of weights (length features + 1), \n",
    "     higher/lower = performance measure  (if feat * w < W we run the 'lower' method)\n",
    "     Returns total extra time for running the slower algorithm\"\"\"\n",
    "    v = np.array(feat).dot(np.array(w)[:-1])\n",
    "    return sum(  max(0,low-high) if v[i] < 0 # extra time for low (Primal) algorithm if this is slower\n",
    "               else max(0,high-low) # extra time to run the Dual (high) alg. if this is slower\n",
    "        for i,(low,high) in enumerate(zip(lower,higher)) )\n",
    "def SVMcount(feat,w,lower=data['PrimalT'],higher=data['DualT']):\n",
    "    \"\"\"Input: feat = matrix of features, w - list/array of weights (length features + 1), \n",
    "     higher/lower = performance measure  (if feat * w < W we run the 'lower' method)\n",
    "     Returns total extra time for running the slower algorithm\"\"\"\n",
    "    v = np.array(feat).dot(np.array(w)[:-1])\n",
    "    return sum( 1 for i,(low,high) in enumerate(zip(lower,higher))\n",
    "               if (low-high)*v[i] < 0 ) # mis-classified if low > high & v[i]<0 or low > high & v[i]>0\n",
    "\n",
    "print(\"Results from training on whole data\")\n",
    "print(\"Effectiveness of solution: %.2f viol, %.2f ticks, %d misclassified\"%(\n",
    "    SVMviolation(features,soln),SVMextraT(features,soln),SVMcount(features,soln)))\n",
    "print(\"Expanded feature set soln: %.2f viol, %.2f ticks, %d misclassified\"%(\n",
    "    SVMviolation(expFeat,expSoln),SVMextraT(expFeat,expSoln),SVMcount(expFeat,expSoln)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now partition your data into 4 subsets of about 21 instances each. Take each subset in turn as the test data and use the remainder to train (fit) the SVM. How well, on average, does this approach work for data it hasn't been trained on?\n",
    "\n",
    "Note: while in general one might want to do the splitting randomly, this could lead to somewhat misleading results. The data sets are very variable in size & complexity, so it would make sense to try to split them a bit more systematically to ensure none of the groups only have very big/complex problems or only small trivial data sets. If you are feeling creative, make up a way to split the instances that deterministic or less random and more likely to be balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# do things correctly using a subset of data for training only\n",
    "def SVMtest(feat,nGroup=4,method=SVM,lower=data['PrimalT'],higher=data['DualT']):\n",
    "    \"\"\"Split features feat into nGroup groups, train using method, evaluate using lower/higher\"\"\"\n",
    "    rows = [i for i in feat.index]\n",
    "    random.seed(9999) # to make the split repeatable\n",
    "    random.shuffle(rows)\n",
    "    step = len(rows)//nGroup+1\n",
    "    grps = [ rows[i:i+step] for i in range(0,len(rows),step)]\n",
    "    print(\"#\"*10,\"Group lengths\",[len(g) for g in grps])\n",
    "    v,t,c = 0,0,0\n",
    "    for g in range(nGroup):\n",
    "        train = sum( (grps[i] for i in range(nGroup) if i!=g), []) # join all groups\n",
    "        test = grps[g]\n",
    "        w = method(feat.loc[train],lower[train] < higher[train])\n",
    "        v += SVMviolation(feat.loc[test],w,lower[test],higher[test])\n",
    "        t += SVMextraT(feat.loc[test],w,lower[test],higher[test])\n",
    "        c += SVMcount(feat.loc[test],w,lower[test],higher[test])\n",
    "    print(\"Performance with\",nGroup,\"groups:\"\n",
    "          \"%.2f viol, %.2f ticks, %d misclassified\"%(v,t,c))\n",
    "    return v,t,c\n",
    "print(\"Results when using original features: %.2f violation, %.2f ticks, %d misclassified\"%\n",
    "      SVMtest(features))\n",
    "print(\"Results when using expanded features: %.2f violation, %.2f ticks, %d misclassified\"%\n",
    "    SVMtest(expFeat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension exercises\n",
    "* We could modify our SVM approach to use an objective that better matches what we want to achieve (e.g. minimise additional computational effort from misclassified instances)\n",
    "* Use the sklearn builtin method `LinearSVC` to implement a support vector machine. Documentation is available [here](https://scikit-learn.org/stable/modules/svm.html) The basic approach is:\n",
    "```python\n",
    "from sklearn import svm\n",
    "mdl = svm.LinearSVC()\n",
    "mdl.fit(X,y) # X = training data, y is -1 when PrimalT < DualT and +1 otherwise\n",
    "mdl.predict(x) # predict outcome using test data x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Getting the data from log files\n",
    "\n",
    "**Note:** This section provided only as background to give you some idea of where the data came from.\n",
    "\n",
    "#### Obtaining data from log files\n",
    "In an ideal world the data would already be sitting in a database or simple CSV file for you to read. However this may not always be possible. It is not uncommon for the data to be sitting in text files (perhaps the log files produced as the algorithms are run). Since such text files can get quite large, it makes sense to compress these. \n",
    "\n",
    "You can then process these log files using Python's string processing capabilities (including regular expressions) to extract what you really want. See the `TextProcessing.ipynb` exercise. Here we are going to skip this, but if you wanted to play with this aspects you can obtain the raw data as follows. That data is available as a zip file at http://users.monash.edu/~andrease/Files/Other/cplexLP.zip. You could just download this, unzip it and look at it - but we want to do things with python here. The basic library functions you need are\n",
    "* `urllib` using the `urllib.request.urlopen(\"name.of.url/to-load\")` function. Basic usage information is provided in this [how-to](https://docs.python.org/3.6/howto/urllib2.html?highlight=get%20url). It is probably easiest to just read this data and write it to file.\n",
    "* `zipfile` module provides methods for reading (and writing) zip archives. The basic usage is to create `zip=ZipFile(<file>,'r')` archive, then use `zip.namelist()` to inspect the contents and `zip.open(name,'r')` to open the named file. Complete documentation is [here](https://docs.python.org/3/library/zipfile.html)\n",
    "* Note that all log files are ASCII. To convert a binary (ASCII) string to the standard python unicode string use `.decode(\"utf-8\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "location = \"http://users.monash.edu/~andrease/Files/Other/cplexLP.zip\"\n",
    "open(\"cplexLP.zip\",\"bw\").write(urlopen(location).read())\n",
    "zipData = ZipFile(\"cplexLP.zip\",\"r\")\n",
    "print(\"Zip file contains\",\"; \".join(zipData.namelist()[:4]),\"...\")\n",
    "if False: # this is too verbose - let's not print it\n",
    "    print(zipData.open(zipData.namelist()[1],\"r\").read().decode(\"utf-8\")) # arbitary example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Features\n",
    "For each instance we want to extract the following information:\n",
    "* The name (identifier) for the data set. This is encoded in the name of each file in the Zip archive (after the cplexLP/) or in the log file itself as part of the \"Problem name\"\n",
    "* Characteristics or *features* of the instance to be solved. The log file contains some \"statistics\" of the problem such as the number of constraints and variables, number of non-zero coefficients, etc. See the CPLEX documentation on [displaying problem statistics](https://www.ibm.com/support/knowledgecenter/SSSA5P_12.8.0/ilog.odms.cplex.help/CPLEX/GettingStarted/topics/tutorials/InteractiveOptimizer/displayingProbStats.html) for more informaiton\n",
    "* The solution algorithm used: this is again shown in the \"Problem name\" or can be seen as an integer in the log file as \"method for linear optimisation\" (1=primal, 2=dual, 4=barrier)\n",
    "* The time required using the solution algorithm: You could take the solution time in seconds, but for the analysis here using the \"Deterministic time\" in \"ticks\" is likely to be more useful as a way of assessing which algorithm is more efficient. Having said that, you may also want to extract the time in seconds. See this brief [discussion on ticks](http://www.thequestforoptimality.com/deterministic-behavior-of-cplex-ticks-or-seconds/) or CPLEX [documentation](https://www.ibm.com/support/knowledgecenter/SSSA5P_12.8.0/ilog.odms.cplex.help/CPLEX/Parameters/topics/DetTiLim.html) for a little more information about what these mysterious ticks mean\n",
    "\n",
    "In particular from the section of the log files that looks like this:\n",
    "```\n",
    "Objective sense      : Minimize\n",
    "Variables            :   18380  [Fix: 7282,  Box: 11098]\n",
    "Objective nonzeros   :       2\n",
    "Linear constraints   :     576  [Less: 486,  Equal: 90]\n",
    "  Nonzeros           :  109706\n",
    "  RHS nonzeros       :      30\n",
    "\n",
    "Variables            : Min LB: 0.000000         Max UB: 212.0000       \n",
    "Objective nonzeros   : Min   : 51.00000         Max   : 100.0000       \n",
    "Linear constraints   :\n",
    "  Nonzeros           : Min   : 1.000000         Max   : 224.0000       \n",
    "  RHS nonzeros       : Min   : 1.000000         Max   : 1.000000     \n",
    "\n",
    "...lines deleted...\n",
    "\n",
    "Deterministic time = 296.40 ticks  (956.13 ticks/sec)\n",
    "```\n",
    "We want to extract: the fields: VarCnt, VarFix,VarBox, ObjNZ,ConNZ, ConLess, ConEqual, ConNZ, RhsNZ, VarMin,VarMax,ObjMin,ObjMax,ConMin,ConMax,RhsMin,RhsMax,PrimalT (or DualT or BarrierT for the ticks)\n",
    "\n",
    "#### Extracting the data\n",
    "\n",
    "Have a go at extracting some/all of the data. Store the data in a pandas `DataFrame` object. See the [10 minute introduction](http://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html) to Pandas if you have not come across this before. Essentially it is a \"table like\" data structure that makes it easy to manipulate data. \n",
    "\n",
    "We want to extract the following fields relating to Variables (Var), Constraints (Con) or the Objective function (Obj):  VarCnt, VarFix,VarBox, ObjNZ,ConNZ, ConLess, ConEqual, ConNZ, RhsNZ, VarMin,VarMax,ObjMin,ObjMax,ConMin,ConMax,RhsMin,RhsMax.\n",
    "We also want the performance of the algorithm: PrimalT or DualT or BarrierT for the ticks.\n",
    "\n",
    "For this exercise there is no need to get all of the fields that might be interesting them all (as getting them all can be a bit fiddly). To get a complete data set we will load a DataFrame of results extracted from these files below.\n",
    "\n",
    "#### Storing it all as a Data frame\n",
    "\n",
    "Have a go at extracting some/all of the data. Store the data in a pandas `DataFrame` object. See the [10 minute introduction](http://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html) to Pandas if you have not come across this before. Essentially it is a \"table like\" data structure that makes it easy to manipulate data. \n",
    "\n",
    "We want to extract the following fields relating to Variables (Var), Constraints (Con) or the Objective function (Obj):  VarCnt, VarFix,VarBox, ObjNZ,ConNZ, ConLess, ConEqual, ConNZ, RhsNZ, VarMin,VarMax,ObjMin,ObjMax,ConMin,ConMax,RhsMin,RhsMax.\n",
    "We also want the performance of the algorithm: PrimalT or DualT or BarrierT for the ticks.\n",
    "\n",
    "For this exercise there is no need to get all of the fields that might be interesting them all (as getting them all can be a bit fiddly). To get a complete data set we will load a DataFrame of results extracted from these files below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,pandas\n",
    "import numpy as np\n",
    "\n",
    "probrex = re.compile(r\"cplexLP/([^.]+)\\.([a-z]+)\\.log\")\n",
    "Instance = { probrex.match(d).group(1):{} for d in zipData.namelist()}\n",
    "for d in zipData.namelist(): \n",
    "    m=probrex.match(d)\n",
    "    Instance[m.group(1)][m.group(2)]=d\n",
    "cntrex = re.compile(r\"^(Variables|Linear)[^:]+: *(\\d+) *(\\[.*\\])? *$\")\n",
    "floatmap={\"all infinite\":1e20,\"all zero\":0.0}\n",
    "floatrex = r\"-?\\d+\\.?\\d*e?[-+0-9]*|all infinite|all zero\" # cover the rare case of infinity\n",
    "rangerex=re.compile(r\"^ *(\\w+).*: +Min.*: +(\"+floatrex+r\") +Max.*: +(\"+floatrex+\")\")\n",
    "rangemap={\"Variables\":\"Var\",\"Objective\":\"Obj\",\"Nonzeros\":\"Con\",\"RHS\":\"Rhs\"}\n",
    "nzmap = {\"Objective\":\"ObjNZ\", \"Nonzeros\":\"ConNZ\",\"RHS\" : \"RhzNZ\" }\n",
    "nzrex = re.compile(r\"^ *(\"+\"|\".join(nzmap.keys())+r\")[^:]*: *(\\d+)$\")\n",
    "data = {} # list of records\n",
    "for inst in Instance:\n",
    "    res = {} # result\n",
    "    continuation = \"\"\n",
    "    for line in zipData.open(Instance[inst][\"primal\"],\"r\"): # read the statistics\n",
    "        line = continuation + line.decode(\"utf-8\") # in case line is continuing\n",
    "        continuation = \"\"\n",
    "        if \"[\" in line and not \"]\" in line:\n",
    "            continuation = line.strip()\n",
    "            continue\n",
    "        m = cntrex.match(line)\n",
    "        if m:\n",
    "            base = \"Var\" if m.group(1)==\"Variables\" else \"Con\"\n",
    "            res[base+\"Cnt\"] = int(m.group(2))\n",
    "            if m.group(3):\n",
    "                for name,val in [i.split(\": \") for i in re.split(\", *\",m.group(3).strip(\"[]\"))]:\n",
    "                    res[base+name]=int(val)\n",
    "            continue\n",
    "        m = rangerex.match(line)\n",
    "        if m:\n",
    "            base = rangemap[m.group(1)]\n",
    "            res[base+\"Min\"] = float(-floatmap[m.group(2)] if \"all\" in m.group(2) else m.group(2))\n",
    "            res[base+\"Max\"] = float(floatmap[m.group(3)] if \"all\" in m.group(3) else m.group(3))\n",
    "            continue\n",
    "        m = nzrex.match(line)\n",
    "        if m: res[nzmap[m.group(1)]] = int(m.group(2))\n",
    "    # end loop over lines in primal data file\n",
    "    for method in [\"primal\",\"dual\",\"barrier\"]:\n",
    "        res[method.capitalize()+\"T\"]= float([\n",
    "            line for line in zipData.open(Instance[inst][method]) # read every line\n",
    "            if line.startswith(b\"Deterministic time = \")][0] #only pick the (one and only) matching line\n",
    "        .split()[3]) # 3rd field of line is ticks\n",
    "    data[inst] = res\n",
    "data = pandas.DataFrame.from_dict(data,orient=\"index\") # could just do DataFrame(data) but that would give us the transpose of what we want\n",
    "# Now just fix up missing values for the various counts and make sure they are integers\n",
    "for key in data: \n",
    "    if key[:3] in [\"Var\",\"Con\"] and key[-3:] not in [\"Min\",\"Max\"]: \n",
    "        data[key].fillna(0,inplace=True)\n",
    "        data[key] = data[key].astype(int) # integer type\n",
    "data.head() # display data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's just check for missing values\n",
    "#data[np.logical_or.reduce(data.isnull(),axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
